{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection / Feature Engineer: Why Does It Work\n",
    "\n",
    "In this notebook, we argue that feature selection and feature engineer may povide crucial component for an end to end machine learning project. We present with theoretical and empirical results correct model specification can be achieved using proposed feature selection and feature engineer methodology. Only then will consctructed classification rule makes sense in both interpretability and also prediction performance.\n",
    "\n",
    "All rights reserved by Professor Shaw-hwa Lo and more information can be found on his [site](https://confluence.columbia.edu/confluence/display/statgene/Shaw-Hwa+Lo). Related papers please refer to Lo (2002, 2009, 2012).\n",
    "\n",
    "### Motivation\n",
    "\n",
    "we are interested to select the most important (influential, predictive) variables out of vast amount of noisy variables under small amount of sample size in a data. We are also interested in mechanically engineer new feature that recover the information preserved in a variable module.\n",
    "\n",
    "### Feature Selection\n",
    "\n",
    "Given $X$ and $Y$, let us define influence score (or I-score) to be the following\n",
    "$$\n",
    "\\text{I}(X,Y) := \\sum_{j \\in \\Pi} n_j^2 (\\bar{Y}_j - \\bar{Y})^2\n",
    "$$\n",
    "while $j$ indicates the $j^\\text{th}$ partition in $\\Pi$ which is the overall partition generated by $X$. \n",
    "\n",
    "If size of number of parameters gets extremely large, we also introduce a greedy search algorithm called Backward Dropping Algorithm (short for BDA). The Backward Dropping Algorithm states the following:\n",
    "- Randomly select $k$ variables each round of Backward Dropping Algorithm;\n",
    "- Take turns and drop each variable of the $k$ variables; and compute I-score; \n",
    "- Drop the variable that leads to the highest I-score, here we have $k-1$ variables left;\n",
    "- Go back to first step and start again.\n",
    "\n",
    "In the end, we report the variable set with the highest I-score and its associated I-score.\n",
    "\n",
    "### Feature Engineer\n",
    "\n",
    "In addition, let us also construct engineered features based on interaction-based variable sets. In other words, given $X$, we can construct\n",
    "$$\n",
    "X^{\\dagger} := \\bar{y}_j, \\forall j \\in \\Pi\n",
    "$$\n",
    "while $\\Pi$ is the total possible partitions generated by selected variable sets $X$ and $j$ indicates the $j^{\\text{th}}$ partition in $\\Pi$. The values of $X^{\\dagger}$ is replaced with $\\bar{y}_j$ which is the local average of resposne variable from each partition $j$.\n",
    "\n",
    "### Artificial Example\n",
    "\n",
    "Let us draw random variables from Bernoulli distribution and create data $X_1, ..., X_p$ and define underyling model to be\n",
    "$$y = \\left\\{\n",
    "\\begin{matrix}\n",
    "X_1 + X_2 & (\\text{mod } 2) \\\\\n",
    "X_3 + X_4 + X_5 & (\\text{mod } 2) \\\\\n",
    "\\end{matrix}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "The goal of this example is correct model specification. We want to capture the important information and in this case we want the two variable modules. If we can successfully capture the important variable modules, we do not even need to worry about what type of machine learning algorithm to choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import bernoulli\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 30)\n",
      "   0  1  2  3  4  5  6  7  8  9  ...  20  21  22  23  24  25  26  27  28  29\n",
      "0  1  0  1  0  0  0  1  1  0  0  ...   0   0   0   1   1   1   1   1   1   0\n",
      "1  1  0  0  0  1  1  1  1  1  0  ...   0   1   1   0   1   1   1   0   0   1\n",
      "\n",
      "[2 rows x 30 columns]\n",
      "0.489\n",
      "0.4985\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "N = n + 1000\n",
    "cutoff = round(n/N, 1)\n",
    "p = 30\n",
    "data_bern = bernoulli.rvs(size=N*p, p=0.5)\n",
    "\n",
    "X = pd.DataFrame(data_bern.reshape([N, p]), columns=np.arange(p).astype(str))\n",
    "print(X.shape)\n",
    "print(X.head(2))\n",
    "\n",
    "I = bernoulli.rvs(size=N, p=0.5)\n",
    "print(np.mean(I))\n",
    "y1 = np.mod(X.iloc[:, 1] + X.iloc[:, 2], 2)\n",
    "y2 = np.mod(X.iloc[:, 2] + X.iloc[:, 3] + X.iloc[:, 4], 2)\n",
    "y = np.where(I == 1, y1, y2)\n",
    "print(np.mean(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "        ################ Interaction-based Learning Statistical Package #################\n",
      "        ############## © All rights reserved with Professor Shawhwa Lo ##################\n",
      "        ##### Site: http://stat.columbia.edu/department-directory/name/shaw-hwa-lo/ #####\n",
      "        \n",
      "----------------------------------------------------------------------------------------------------\n",
      "README:\n",
      "This script has the following functions:\n",
      "\n",
      "    (1) iscore(): this function computes the I-score of selected X at predicting Y\n",
      "    (2) BDA(): this function runs through Backward Dropping Algorithm once\n",
      "    (3) InteractionLearning(): this function runs many rounds of BDA and \n",
      "                               finalize the variables selcted according to I-score\n",
      "    \n",
      "ACKNOWLEDGEMENT:\n",
      "This script is not-fot-profit and it is a production of my research \n",
      "during my time at Columbia University.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.InteractionBasedLearning.InteractionLearning(newX, y, testSize=0.3, num_initial_draw=7, total_rounds=10, top_how_many=3, nameExists=True, TYPE=<class 'int'>, verbatim=True)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run \"../scripts/InteractionBasedLearning.py\"\n",
    "InteractionBasedLearning.InteractionLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:02<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Consumption (in sec): 62.85\n",
      "Time Consumption (in min): 1.05\n",
      "Time Consumption (in hr): 0.02\n"
     ]
    }
   ],
   "source": [
    "tmpResult = InteractionBasedLearning.InteractionLearning(\n",
    "    newX=X,\n",
    "    y=y,\n",
    "    testSize=cutoff,\n",
    "    num_initial_draw=9,\n",
    "    total_rounds=200,\n",
    "    top_how_many=2,\n",
    "    nameExists=True,\n",
    "    TYPE=str,\n",
    "    verbatim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modules</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>[[1, 2]]</td>\n",
       "      <td>29.118667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>[[2, 3, 4]]</td>\n",
       "      <td>16.687193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[[17, 23, 26]]</td>\n",
       "      <td>1.498895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>[[23, 26]]</td>\n",
       "      <td>1.431386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>[[4, 21]]</td>\n",
       "      <td>1.403558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Modules      Score\n",
       "170        [[1, 2]]  29.118667\n",
       "80      [[2, 3, 4]]  16.687193\n",
       "122  [[17, 23, 26]]   1.498895\n",
       "159      [[23, 26]]   1.431386\n",
       "58        [[4, 21]]   1.403558"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpResult['Brief'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.735238</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.778243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.242545</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.257322</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.265823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.242545</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.740891</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.736842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  2         0  2  3  4         0\n",
       "0  0  1  0.735238  1  0  0  0.778243\n",
       "1  0  0  0.242545  0  0  1  0.736842\n",
       "2  1  1  0.257322  1  0  1  0.265823\n",
       "3  0  0  0.242545  0  0  0  0.152000\n",
       "4  1  0  0.740891  0  0  1  0.736842"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpResult['New Data'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning: Building Classifier\n",
    "\n",
    "To test our idea about the importance of feature selection and feature engineer, we build classifier using a 3-layer neural network using original data set. Then we build classifier using the same 3-layer neural network architecture using important variable moduels and interaction-based features. In the end, we observe that proposed method deliver test set performance similar to that of theoretical prediction while original method does not hit the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------\n",
      "\n",
      "        Yin's Deep Learning Package \n",
      "        Copyright © YINS CAPITAL, 2009 – Present\n",
      "        For more information, please go to www.YinsCapital.com\n",
      "        \n",
      "---------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%run \"../scripts/YinsDL.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.YinsDL.NN3_Classifier(X_train, y_train, X_test, y_test, l1_act='relu', l2_act='relu', l3_act='softmax', layer1size=128, layer2size=64, layer3size=2, optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'], num_of_epochs=10, plotROC=True, verbose=True)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YinsDL.NN3_Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try using original data first.\n",
    "\n",
    "- We use a simple 3-layer neural network architecture first;\n",
    "- Then we use a more comlicated 10-layer neural network architecture to test again.\n",
    "\n",
    "We print the performance results in both L1 loss and area under curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=cutoff, random_state=0)\n",
    "print('Shape of X training set:', X_train.shape, 'Shape of X in test set:', X_test.shape)\n",
    "print('First few obs of y:', y_train[:5])\n",
    "\n",
    "testresult = YinsDL.NN3_Classifier(X_train, y_train, X_test, y_test, \n",
    "                                 l1_act='relu', l2_act='relu', l3_act='softmax',\n",
    "                                 layer1size=128, layer2size=64, layer3size=2,\n",
    "                                 num_of_epochs=50, plotROC=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = YinsDL.NN10_Classifier(\n",
    "        X_train, y_train, X_test, y_test, \n",
    "        l1_act='relu', l2_act='relu', l3_act='relu', l4_act='relu', l5_act='relu',\n",
    "        l6_act='relu', l7_act='relu', l8_act='relu', l9_act='relu', l10_act='softmax',\n",
    "        layer1size=128, layer2size=64, layer3size=64, layer4size=64, layer5size=64, \n",
    "        layer6size=64, layer7size=64, layer8size=64, layer9size=64, layer10size=2,\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "        num_of_epochs=50,\n",
    "        plotROC=True,\n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, let us dare to attempt convlutional neural network with one convolutional layer and three neural network layers. This architecture might be too complicated for this data and hence I expect poor performance.\n",
    "\n",
    "Please notice that *inputSHAPEwidth* and *inputSHAPElength* needs to multiply to be exactly the number of features. Otherwise the algorithm cannot successfully reshape an observation into grid structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnTmpResult = YinsDL.C1NN3_Classifier(\n",
    "        X_train, y_train, X_test, y_test, \n",
    "        inputSHAPEwidth=10, inputSHAPElenth=3,\n",
    "        filter1 = [[1,0], [0,1]],\n",
    "        l1_act='relu', l2_act='relu', l3_act='softmax',\n",
    "        layer1size=128, layer2size=64, layer3size=2,\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "        num_of_epochs=50,\n",
    "        plotROC=True,\n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the filter is defined as \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "and this is completely arbitrary. Though the algorithm runs fine, the prediction results has not been great. This is because as the variable $[X_1, X_2, X_3, X_4]$ line up and are inserted in a square. The filter will actually pick up $X_1$ and $X_3$ which is not quite the correct model. \n",
    "\n",
    "However, we can design the following filter\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "and this should be able to pick up some signal due to the positioning of variables in the grid. However, this observation is not replicable because I already know what the data looks like. In reality, one needs high computing power to search for the ideal filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnTmpResult = YinsDL.C1NN3_Classifier(\n",
    "        X_train, y_train, X_test, y_test, \n",
    "        inputSHAPEwidth=10, inputSHAPElenth=3,\n",
    "        filter1 = [[1,1], [1,0]],\n",
    "        l1_act='relu', l2_act='relu', l3_act='softmax',\n",
    "        layer1size=128, layer2size=64, layer3size=2,\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "        num_of_epochs=50,\n",
    "        plotROC=True,\n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also upgrade our algorithm to an even more advanced version: an architecture with two convolutional layer and three neural network layer, i.e. C2NN3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnTmpResult = YinsDL.C2NN3_Classifier(\n",
    "        pd.concat([X_train, X_train, X_train], axis=1), y_train, \n",
    "        pd.concat([X_test, X_test, X_test], axis=1), y_test, \n",
    "        inputSHAPEwidth1=9, inputSHAPElenth1=10,\n",
    "        inputSHAPEwidth2=7, inputSHAPElenth2=8,\n",
    "        filter1 = [[1,1,1], [1,0,0], [1,0,0]], filter2 = [[1,1], [1,0]],\n",
    "        l1_act='relu', l2_act='relu', l3_act='softmax',\n",
    "        layer1size=128, layer2size=64, layer3size=2,\n",
    "        optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'],\n",
    "        num_of_epochs=100,\n",
    "        plotROC=True, verbose=True, printManual=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try using important variable modules and interaction-based features as input data.\n",
    "\n",
    "- Just as the previous example with entire data set, we use the same 3-layer neural network architecture;\n",
    "- Next we use the same 10-layer neural network architecture.\n",
    "\n",
    "We print the test performance in both accuracy computed by L1 loss and area under curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tmpResult['New Data'], y, test_size=cutoff, random_state=0)\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train[:5])\n",
    "\n",
    "testresult = YinsDL.NN3_Classifier(\n",
    "    X_train, y_train, X_test, y_test, \n",
    "    l1_act='relu', l2_act='relu', l3_act='softmax',\n",
    "    layer1size=128, layer2size=64, layer3size=2,\n",
    "    num_of_epochs=50, plotROC=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = YinsDL.NN10_Classifier(\n",
    "        X_train, y_train, X_test, y_test, \n",
    "        l1_act='relu', l2_act='relu', l3_act='relu', l4_act='relu', l5_act='relu',\n",
    "        l6_act='relu', l7_act='relu', l8_act='relu', l9_act='relu', l10_act='softmax',\n",
    "        layer1size=128, layer2size=64, layer3size=64, layer4size=64, layer5size=64, \n",
    "        layer6size=64, layer7size=64, layer8size=64, layer9size=64, layer10size=2,\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "        num_of_epochs=50,\n",
    "        plotROC=True,\n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnTmpResult = YinsDL.C1NN3_Classifier(\n",
    "        pd.concat([X_train, X_train, X_train], axis=1), y_train, \n",
    "        pd.concat([X_test, X_test, X_test], axis=1), y_test, \n",
    "        inputSHAPEwidth=7, inputSHAPElenth=3,\n",
    "        filter1 = [[1,0], [0,1]],\n",
    "        l1_act='relu', l2_act='relu', l3_act='softmax',\n",
    "        layer1size=128, layer2size=64, layer3size=2,\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "        num_of_epochs=50,\n",
    "        plotROC=True,\n",
    "        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is 77%. If this experiment is repeated many times, we should be able to hit average prediction rate of 75%. Why 75%? This is the exact theoretical prediction rate of the artificial example. In the underlying model $Y$, there are two modules. One of the correct module gives us at least 50% to start with. Since there is no marginal signal, the first module will perform 50% on the rest of the observations. This means correct theoretical prediction rate (the best you can do) is $75\\% = 50\\% + 50\\% \\times 50\\%$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "As a conclusion, let us put everything together for performance comparison on the test set.\n",
    "\n",
    "| Data (Which Variables To Use) | Algorithm | Performance  |\n",
    "| -------------------------- |:---------:| ------------:|\n",
    "| All the Original Data      | NN3       | 0.62 |\n",
    "| All the Original Data      | NN10      | 0.60 |\n",
    "| All the Original Data + Convolutional Features | C1NN3 (filter1)     | 0.51 |\n",
    "| All the Original Data + Convolutional Features | C1NN3 (filter2)     | 0.60 |\n",
    "| All the Original Data + Convolutional Features | C2NN3 (filter3)     | 0.60 |\n",
    "| $[X_1, X_2], X^{\\dagger}_{\\{X_1, X_2\\}}, [X_2, X_3, X_4], X^{\\dagger}_{\\{X_2, X_3, X_4\\}}$ | NN3 | 0.75 |\n",
    "| $[X_1, X_2], X^{\\dagger}_{\\{X_1, X_2\\}}, [X_2, X_3, X_4], X^{\\dagger}_{\\{X_2, X_3, X_4\\}}$ | NN10 | 0.75 |\n",
    "| $[X_1, X_2], X^{\\dagger}_{\\{X_1, X_2\\}}, [X_2, X_3, X_4], X^{\\dagger}_{\\{X_2, X_3, X_4\\}}$ | C1NN3 | 0.75 |\n",
    "| -------------------------- | --------- | ------------ |\n",
    "| Theoretical Rate (Under Correct Model Specification)      | Any       | 0.75 |\n",
    "| -------------------------- | --------- | ------------ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigation ends here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
