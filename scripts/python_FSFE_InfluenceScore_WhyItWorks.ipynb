{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection / Feature Engineer: Why Does It Work\n",
    "\n",
    "Motivation: we are motivated to select the most important (influential, predictive) variables out of vast amount of noisy variables under small amount of sample size in a data. We are also interested in mechanically engineer new feature that recover the information preserved in a variable module.\n",
    "\n",
    "### Feature Selection\n",
    "\n",
    "Given $X$ and $Y$, let us define influence score (or I-score) to be the following\n",
    "$$\n",
    "\\text{I}(X,Y) := \\sum_{j \\in \\Pi} n_j^2 (\\bar{Y}_j - \\bar{Y})^2\n",
    "$$\n",
    "while $j$ indicates the $j^\\text{th}$ partition in $\\Pi$ which is the overall partition generated by $X$. \n",
    "\n",
    "If size of number of parameters gets extremely large, we also introduce a greedy search algorithm called Backward Dropping Algorithm (short for BDA). The Backward Dropping Algorithm states the following:\n",
    "- Randomly select $k$ variables each round of Backward Dropping Algorithm;\n",
    "- Take turns and drop each variable of the $k$ variables; and compute I-score; \n",
    "- Drop the variable that leads to the highest I-score, here we have $k-1$ variables left;\n",
    "- Go back to first step and start again.\n",
    "\n",
    "In the end, we report the variable set with the highest I-score and its associated I-score.\n",
    "\n",
    "### Feature Engineer\n",
    "\n",
    "In addition, let us also construct engineered features based on interaction-based variable sets. In other words, given $X$, we can construct\n",
    "$$\n",
    "X^{\\dagger} := \\bar{y}_j, \\forall j \\in \\Pi\n",
    "$$\n",
    "while $\\Pi$ is the total possible partitions generated by selected variable sets $X$ and $j$ indicates the $j^{\\text{th}}$ partition in $\\Pi$. The values of $X^{\\dagger}$ is replaced with $\\bar{y}_j$ which is the local average of resposne variable from each partition $j$.\n",
    "\n",
    "### Artificial Example\n",
    "\n",
    "Let us draw random variables from Bernoulli distribution and create data $X_1, ..., X_p$ and define underyling model to be\n",
    "$$y = \\left\\{\n",
    "\\begin{matrix}\n",
    "X_1 + X_2 & (\\text{mod } 2) \\\\\n",
    "X_3 + X_4 + X_5 & (\\text{mod } 2) \\\\\n",
    "\\end{matrix}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "The goal of this example is correct model specification. We want to capture the important information and in this case we want the two variable modules. If we can successfully capture the important variable modules, we do not even need to worry about what type of machine learning algorithm to choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import bernoulli\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 30)\n",
      "   0  1  2  3  4  5  6  7  8  9  ...  20  21  22  23  24  25  26  27  28  29\n",
      "0  1  0  1  0  0  0  1  1  1  0  ...   0   0   0   0   0   0   0   0   1   0\n",
      "1  0  0  1  0  1  1  0  0  0  0  ...   0   0   0   0   1   0   0   1   0   1\n",
      "\n",
      "[2 rows x 30 columns]\n",
      "0.5175\n",
      "0.5085\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "N = n + 1000\n",
    "cutoff = round(n/N, 1)\n",
    "p = 30\n",
    "data_bern = bernoulli.rvs(size=N*p, p=0.5)\n",
    "\n",
    "X = pd.DataFrame(data_bern.reshape([N, p]), columns=np.arange(p).astype(str))\n",
    "print(X.shape)\n",
    "print(X.head(2))\n",
    "\n",
    "I = bernoulli.rvs(size=N, p=0.5)\n",
    "print(np.mean(I))\n",
    "y1 = np.mod(X.iloc[:, 1] + X.iloc[:, 2], 2)\n",
    "y2 = np.mod(X.iloc[:, 2] + X.iloc[:, 3] + X.iloc[:, 4], 2)\n",
    "y = np.where(I == 1, y1, y2)\n",
    "print(np.mean(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------\n",
      "\n",
      "        Yin's Interaction-based Learning Statistical Package \n",
      "        Copyright © YINS CAPITAL, 2009 – Present\n",
      "        For more information, please go to www.YinsCapital.com\n",
      "        \n",
      "README:\n",
      "This script has the following functions:\n",
      "\n",
      "    (1) iscore(): this function computes the I-score of selected X at predicting Y\n",
      "    (2) BDA(): this function runs through Backward Dropping Algorithm once\n",
      "    (3) InteractionLearning(): this function runs many rounds of BDA and \n",
      "                               finalize the variables selcted according to I-score\n",
      "    \n",
      "---------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.InteractionBasedLearning.InteractionLearning>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run \"../scripts/InteractionBasedLearning.py\"\n",
    "InteractionBasedLearning.InteractionLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 200/200 [04:37<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Consumption (in sec): 278.11\n",
      "Time Consumption (in min): 4.64\n",
      "Time Consumption (in hr): 0.08\n"
     ]
    }
   ],
   "source": [
    "tmpResult = InteractionBasedLearning.InteractionLearning(\n",
    "    newX=X,\n",
    "    y=y,\n",
    "    testSize=cutoff,\n",
    "    num_initial_draw=9,\n",
    "    total_rounds=200,\n",
    "    top_how_many=2,\n",
    "    nameExists=False,\n",
    "    TYPE=str,\n",
    "    verbatim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modules</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>[[1, 2]]</td>\n",
       "      <td>33.628328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>[[2, 3, 4]]</td>\n",
       "      <td>14.749468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>[[11, 15]]</td>\n",
       "      <td>1.416835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>[[7, 9, 20]]</td>\n",
       "      <td>1.341947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>[[5, 12, 26]]</td>\n",
       "      <td>1.215592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Modules      Score\n",
       "63        [[1, 2]]  33.628328\n",
       "163    [[2, 3, 4]]  14.749468\n",
       "149     [[11, 15]]   1.416835\n",
       "172   [[7, 9, 20]]   1.341947\n",
       "145  [[5, 12, 26]]   1.215592"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpResult['Brief'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.753684</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.732510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.753684</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.232068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.258964</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.743396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.773234</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.756654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.753684</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.232068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  2         0  2  3  4         0\n",
       "0  0  1  0.753684  1  0  0  0.732510\n",
       "1  0  1  0.753684  1  0  1  0.232068\n",
       "2  1  1  0.258964  1  1  1  0.743396\n",
       "3  1  0  0.773234  0  0  1  0.756654\n",
       "4  0  1  0.753684  1  0  1  0.232068"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpResult['New Data'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning: Building Classifier\n",
    "\n",
    "To test our idea about the importance of feature selection and feature engineer, we build classifier using a 3-layer neural network using original data set. Then we build classifier using the same 3-layer neural network architecture using important variable moduels and interaction-based features. In the end, we observe that proposed method deliver test set performance similar to that of theoretical prediction while original method does not hit the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------\n",
      "\n",
      "        Yin's Deep Learning Package \n",
      "        Copyright © YINS CAPITAL, 2009 – Present\n",
      "        For more information, please go to www.YinsCapital.com\n",
      "        \n",
      "---------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%run \"../scripts/YinsDL.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.YinsDL.NN3_Classifier>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YinsDL.NN3_Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try using original data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 30) (1000, 30)\n",
      "[0 0 0 1 0 1 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 0 1 1\n",
      " 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1\n",
      " 0 1 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0\n",
      " 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 0 1 1 0 0 1\n",
      " 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0\n",
      " 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1\n",
      " 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 0 0 0\n",
      " 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 1\n",
      " 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0\n",
      " 1 0 0 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0\n",
      " 1 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 1 0 0 0\n",
      " 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0\n",
      " 1 1 1 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1\n",
      " 0 0 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1\n",
      " 1 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 0\n",
      " 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1\n",
      " 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 1 1 0 0\n",
      " 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 0 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1\n",
      " 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1\n",
      " 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 1 1 0 1 0 1 1 1\n",
      " 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0\n",
      " 0 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1\n",
      " 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0\n",
      " 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 1 1 1\n",
      " 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=cutoff, random_state=0)\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eagle\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 1000 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 0.7204 - accuracy: 0.5240\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s 89us/sample - loss: 0.6268 - accuracy: 0.6560\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.5830 - accuracy: 0.7100\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s 90us/sample - loss: 0.5367 - accuracy: 0.7590\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s 85us/sample - loss: 0.5033 - accuracy: 0.7780\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s 76us/sample - loss: 0.4755 - accuracy: 0.7840\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.4579 - accuracy: 0.7930\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s 87us/sample - loss: 0.4386 - accuracy: 0.8040\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s 90us/sample - loss: 0.4224 - accuracy: 0.8200\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s 86us/sample - loss: 0.4078 - accuracy: 0.8200\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s 85us/sample - loss: 0.3941 - accuracy: 0.8360\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.3801 - accuracy: 0.8480\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s 88us/sample - loss: 0.3699 - accuracy: 0.8470\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s 87us/sample - loss: 0.3590 - accuracy: 0.8510\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s 85us/sample - loss: 0.3440 - accuracy: 0.8610\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.3319 - accuracy: 0.8690\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s 85us/sample - loss: 0.3216 - accuracy: 0.8720\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s 88us/sample - loss: 0.3047 - accuracy: 0.8900\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s 83us/sample - loss: 0.3009 - accuracy: 0.8920\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s 85us/sample - loss: 0.2805 - accuracy: 0.9090\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s 86us/sample - loss: 0.2768 - accuracy: 0.8960\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s 86us/sample - loss: 0.2691 - accuracy: 0.9140\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s 87us/sample - loss: 0.2523 - accuracy: 0.9240\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s 87us/sample - loss: 0.2427 - accuracy: 0.9190\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.2417 - accuracy: 0.9260\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s 88us/sample - loss: 0.2309 - accuracy: 0.9360\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s 90us/sample - loss: 0.2266 - accuracy: 0.9280\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s 92us/sample - loss: 0.2219 - accuracy: 0.9330\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s 95us/sample - loss: 0.2011 - accuracy: 0.9490\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s 89us/sample - loss: 0.1993 - accuracy: 0.9440\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s 102us/sample - loss: 0.1891 - accuracy: 0.9460\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s 93us/sample - loss: 0.1828 - accuracy: 0.9510\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s 89us/sample - loss: 0.1744 - accuracy: 0.9550\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s 83us/sample - loss: 0.1688 - accuracy: 0.9580\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s 90us/sample - loss: 0.1666 - accuracy: 0.9540\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s 83us/sample - loss: 0.1622 - accuracy: 0.9600\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s 88us/sample - loss: 0.1468 - accuracy: 0.9680\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s 89us/sample - loss: 0.1396 - accuracy: 0.9760\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s 90us/sample - loss: 0.1291 - accuracy: 0.9770\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s 91us/sample - loss: 0.1269 - accuracy: 0.9800\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s 96us/sample - loss: 0.1229 - accuracy: 0.9780\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s 119us/sample - loss: 0.1147 - accuracy: 0.9850\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s 123us/sample - loss: 0.1123 - accuracy: 0.9790\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s 114us/sample - loss: 0.1117 - accuracy: 0.9770\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s 91us/sample - loss: 0.1028 - accuracy: 0.9880\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s 85us/sample - loss: 0.1023 - accuracy: 0.9860\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s 95us/sample - loss: 0.0988 - accuracy: 0.9870\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.0874 - accuracy: 0.9910\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s 90us/sample - loss: 0.0854 - accuracy: 0.9900\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s 82us/sample - loss: 0.0793 - accuracy: 0.9970\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "testresult = YinsDL.NN3_Classifier(X_train, y_train, X_test, y_test, \n",
    "                                 l1_act='relu', l2_act='relu', l3_act='softmax',\n",
    "                                 layer1size=128, layer2size=64, layer3size=2,\n",
    "                                 num_of_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'confusion':      0    1\n",
       " 0  320  165\n",
       " 1  179  336, 'test_acc': 0.656}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testresult['Performance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try using important variable modules and interaction-based features as input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 7) (1000, 7)\n",
      "[0 0 0 1 0 1 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 0 0 0 1 1\n",
      " 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1\n",
      " 0 1 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0\n",
      " 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 0 1 1 0 0 1\n",
      " 0 1 1 0 1 0 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0\n",
      " 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1\n",
      " 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 0 0 0 0\n",
      " 0 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1 0 0 0 0 0 1\n",
      " 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0\n",
      " 1 0 0 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0\n",
      " 1 1 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 1 0 1 0 1 0 0 0\n",
      " 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0\n",
      " 1 1 1 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1\n",
      " 0 0 0 1 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1\n",
      " 1 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 0 1 1 1 0 0 0\n",
      " 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 0 1\n",
      " 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 1 1 0 0\n",
      " 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 0 0 0 0 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1\n",
      " 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 1\n",
      " 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 1 1 1 0 1 0 1 1 1\n",
      " 1 0 0 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 0 1 0 0\n",
      " 0 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 1 1\n",
      " 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0\n",
      " 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 1 1 1\n",
      " 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0\n",
      " 0]\n",
      "2.0.0\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 1000 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 0s 203us/sample - loss: 0.5791 - accuracy: 0.7010\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.4574 - accuracy: 0.7500\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s 91us/sample - loss: 0.4115 - accuracy: 0.7470\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s 88us/sample - loss: 0.3944 - accuracy: 0.7550\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s 95us/sample - loss: 0.3837 - accuracy: 0.7460\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s 96us/sample - loss: 0.3968 - accuracy: 0.7230\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s 88us/sample - loss: 0.3715 - accuracy: 0.7410\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s 97us/sample - loss: 0.3751 - accuracy: 0.7550\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s 92us/sample - loss: 0.3743 - accuracy: 0.7500\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s 88us/sample - loss: 0.3752 - accuracy: 0.7300\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.3674 - accuracy: 0.7450\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s 97us/sample - loss: 0.3704 - accuracy: 0.7630\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s 99us/sample - loss: 0.3728 - accuracy: 0.7400\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s 95us/sample - loss: 0.3737 - accuracy: 0.7410\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s 93us/sample - loss: 0.3657 - accuracy: 0.7460\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s 94us/sample - loss: 0.3640 - accuracy: 0.7480\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s 68us/sample - loss: 0.3688 - accuracy: 0.7420\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s 92us/sample - loss: 0.3677 - accuracy: 0.7450\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s 89us/sample - loss: 0.3723 - accuracy: 0.7360\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s 94us/sample - loss: 0.3630 - accuracy: 0.7460\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s 69us/sample - loss: 0.3650 - accuracy: 0.7330\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s 81us/sample - loss: 0.3664 - accuracy: 0.7450\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.3661 - accuracy: 0.7550\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s 91us/sample - loss: 0.3644 - accuracy: 0.7530\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s 69us/sample - loss: 0.3637 - accuracy: 0.7360\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s 94us/sample - loss: 0.3632 - accuracy: 0.7420\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s 97us/sample - loss: 0.3782 - accuracy: 0.7300\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s 100us/sample - loss: 0.3671 - accuracy: 0.7470\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s 91us/sample - loss: 0.3624 - accuracy: 0.7380\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s 98us/sample - loss: 0.3724 - accuracy: 0.7400\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s 76us/sample - loss: 0.3717 - accuracy: 0.7390\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s 96us/sample - loss: 0.3675 - accuracy: 0.7410\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s 92us/sample - loss: 0.3633 - accuracy: 0.7520\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s 102us/sample - loss: 0.3657 - accuracy: 0.7360\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s 76us/sample - loss: 0.3652 - accuracy: 0.7430\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s 93us/sample - loss: 0.3695 - accuracy: 0.7430\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s 88us/sample - loss: 0.3590 - accuracy: 0.7490\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s 95us/sample - loss: 0.3613 - accuracy: 0.7460\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s 84us/sample - loss: 0.3633 - accuracy: 0.7400\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s 94us/sample - loss: 0.3696 - accuracy: 0.7180\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s 92us/sample - loss: 0.3672 - accuracy: 0.7390\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s 96us/sample - loss: 0.3681 - accuracy: 0.7240\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s 87us/sample - loss: 0.3660 - accuracy: 0.7180\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s 101us/sample - loss: 0.3720 - accuracy: 0.7430\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s 102us/sample - loss: 0.3722 - accuracy: 0.7330\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s 107us/sample - loss: 0.3763 - accuracy: 0.7380\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s 94us/sample - loss: 0.3675 - accuracy: 0.7380\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s 90us/sample - loss: 0.3598 - accuracy: 0.7490\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s 93us/sample - loss: 0.3692 - accuracy: 0.7460\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s 77us/sample - loss: 0.3646 - accuracy: 0.7570\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tmpResult['New Data'], y, test_size=cutoff, random_state=0)\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train)\n",
    "\n",
    "testresult = YinsDL.NN3_Classifier(X_train, y_train, X_test, y_test, \n",
    "                                 l1_act='relu', l2_act='relu', l3_act='softmax',\n",
    "                                 layer1size=128, layer2size=64, layer3size=2,\n",
    "                                 num_of_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'confusion':      0    1\n",
       " 0  351  134\n",
       " 1   94  421, 'test_acc': 0.772}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testresult['Performance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is 77%. If this experiment is repeated many times, we should be able to hit average prediction rate of 75%. Why 75%? This is the exact theoretical prediction rate of the artificial example. In the underlying model $Y$, there are two modules. One of the correct module gives us at least 50% to start with. Since there is no marginal signal, the first module will perform 50% on the rest of the observations. This means correct theoretical prediction rate (the best you can do) is $75\\% = 50\\% + 50\\% \\times 50\\%$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigation ends here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
